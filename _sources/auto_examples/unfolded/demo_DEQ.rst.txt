
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/unfolded/demo_DEQ.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        New to DeepInverse? Get started with the basics with the
        :ref:`5 minute quickstart tutorial <sphx_glr_auto_examples_basics_demo_quickstart.py>`.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_unfolded_demo_DEQ.py:


Deep Equilibrium (DEQ) algorithms for image deblurring
====================================================================================================

This a toy example to show you how to use DEQ to solve a deblurring problem.
Note that this is a small dataset for training. For optimal results, use a larger dataset.

For now DEQ is only possible with PGD, HQS and GD optimization algorithms.

.. GENERATED FROM PYTHON SOURCE LINES 11-23

.. code-block:: Python


    import deepinv as dinv
    from pathlib import Path
    import torch
    from torch.utils.data import DataLoader
    from deepinv.optim.data_fidelity import L2
    from deepinv.optim.prior import PnP
    from deepinv.optim import PGD
    from torchvision import transforms
    from deepinv.utils import load_dataset, load_degradation









.. GENERATED FROM PYTHON SOURCE LINES 24-27

Setup paths for data loading and results.
----------------------------------------------------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 27-39

.. code-block:: Python


    BASE_DIR = Path(".")
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    CKPT_DIR = BASE_DIR / "ckpts"
    DEG_DIR = BASE_DIR / "degradations"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 40-43

Load base image datasets and degradation operators.
----------------------------------------------------------------------------------------
In this example, we use the CBSD500 dataset and the Set3C dataset for testing.

.. GENERATED FROM PYTHON SOURCE LINES 43-62

.. code-block:: Python


    img_size = 32
    n_channels = 3  # 3 for color images, 1 for gray-scale images
    operation = "deblurring"
    # For simplicity, we use a small dataset for training.
    # To be replaced for optimal results. For example, you can use the larger "drunet" dataset.
    train_dataset_name = "CBSD500"
    test_dataset_name = "set3c"
    # Generate training and evaluation datasets in HDF5 folders and load them.
    test_transform = transforms.Compose(
        [transforms.CenterCrop(img_size), transforms.ToTensor()]
    )
    train_transform = transforms.Compose(
        [transforms.RandomCrop(img_size), transforms.ToTensor()]
    )
    train_base_dataset = load_dataset(train_dataset_name, transform=train_transform)
    test_base_dataset = load_dataset(test_dataset_name, transform=test_transform)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/CBSD500.zip
      0%|          | 0.00/71.0M [00:00<?, ?iB/s]     12%|█▏        | 8.73M/71.0M [00:00<00:00, 87.3MiB/s]     27%|██▋       | 19.2M/71.0M [00:00<00:00, 97.3MiB/s]     42%|████▏     | 29.7M/71.0M [00:00<00:00, 101MiB/s]      57%|█████▋    | 40.4M/71.0M [00:00<00:00, 103MiB/s]     72%|███████▏  | 51.0M/71.0M [00:00<00:00, 104MiB/s]     87%|████████▋ | 61.6M/71.0M [00:00<00:00, 105MiB/s]    100%|██████████| 71.0M/71.0M [00:00<00:00, 103MiB/s]
    CBSD500 dataset downloaded in datasets
    Downloading datasets/set3c.zip
      0%|          | 0.00/385k [00:00<?, ?iB/s]    100%|██████████| 385k/385k [00:00<00:00, 19.0MiB/s]
    set3c dataset downloaded in datasets




.. GENERATED FROM PYTHON SOURCE LINES 63-66

Generate a dataset of low resolution images and load it.
----------------------------------------------------------------------------------------
We use the Downsampling class from the physics module to generate a dataset of low resolution images.

.. GENERATED FROM PYTHON SOURCE LINES 66-109

.. code-block:: Python



    # Use parallel dataloader if using a GPU to speed up training, otherwise, as all computes are on CPU, use synchronous
    # dataloading.
    num_workers = 4 if torch.cuda.is_available() else 0

    # Degradation parameters
    noise_level_img = 0.03

    # Generate a motion blur operator.
    kernel_index = 1  # which kernel to chose among the 8 motion kernels from 'Levin09.mat'
    kernel_torch = load_degradation("Levin09.npy", DEG_DIR / "kernels", index=kernel_index)
    kernel_torch = kernel_torch.unsqueeze(0).unsqueeze(
        0
    )  # add batch and channel dimensions

    # Generate the gaussian blur downsampling operator.
    physics = dinv.physics.BlurFFT(
        img_size=(n_channels, img_size, img_size),
        filter=kernel_torch,
        device=device,
        noise_model=dinv.physics.GaussianNoise(sigma=noise_level_img),
    )

    my_dataset_name = "demo_DEQ"
    n_images_max = (
        1000 if torch.cuda.is_available() else 10
    )  # maximal number of images used for training
    measurement_dir = DATA_DIR / train_dataset_name / operation
    generated_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_base_dataset,
        test_dataset=test_base_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Levin09.npy degradation downloaded in degradations/kernels
    /home/runner/work/deepinv/deepinv/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
      warnings.warn(warn_msg)
    Dataset has been saved at measurements/CBSD500/deblurring/demo_DEQ0.h5




.. GENERATED FROM PYTHON SOURCE LINES 110-116

Define the  DEQ algorithm.
----------------------------------------------------------------------------------------
We use the  :func:`deepinv.optim.PGD` with the argument `DEQ=True` to defined the DEQ architecture.
The chosen algorithm is here PGD (Proximal Gradient Descent).
Note for DEQ, the prior and regularization parameters should be common for all iterations
to keep a constant fixed-point operator.

.. GENERATED FROM PYTHON SOURCE LINES 116-147

.. code-block:: Python



    # Select the data fidelity term
    data_fidelity = L2()

    # Set up the trainable denoising prior. Here the prior model is common for all iterations. We use here a pretrained denoiser.
    prior = PnP(denoiser=dinv.models.DnCNN(depth=20, pretrained="download").to(device))

    # Unrolled optimization algorithm parameters
    max_iter = 20 if torch.cuda.is_available() else 10
    stepsize = [1.0]  # stepsize of the algorithm
    sigma_denoiser = [0.03]  # noise level parameter of the denoiser
    jacobian_free = False  # does not perform Jacobian inversion.

    trainable_params = [
        "stepsize",
        "sigma_denoiser",
    ]  # define which parameters are trainable. Here the stepsize and noise level of the denoiser are trained.

    # Define the unfolded trainable model.
    model = PGD(
        DEQ=True,
        trainable_params=trainable_params,
        stepsize=stepsize,
        sigma_denoiser=sigma_denoiser,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
        anderson_acceleration=True,
    )








.. GENERATED FROM PYTHON SOURCE LINES 148-151

Define the training parameters.
-------------------------------
We use the Adam optimizer and the StepLR scheduler.

.. GENERATED FROM PYTHON SOURCE LINES 151-177

.. code-block:: Python



    # training parameters
    epochs = 10 if torch.cuda.is_available() else 2
    learning_rate = 1e-4
    train_batch_size = 32 if torch.cuda.is_available() else 1
    test_batch_size = 3


    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8))

    # choose supervised training loss
    losses = [dinv.loss.SupLoss(metric=dinv.metric.MSE())]

    # Logging parameters
    verbose = True

    train_dataloader = DataLoader(
        train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False
    )








.. GENERATED FROM PYTHON SOURCE LINES 178-181

Train the network
-----------------
We train the network using the library's train function.

.. GENERATED FROM PYTHON SOURCE LINES 181-200

.. code-block:: Python


    trainer = dinv.Trainer(
        model=model,
        physics=physics,
        epochs=epochs,
        scheduler=scheduler,
        device=device,
        losses=losses,
        optimizer=optimizer,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        show_progress_bar=True,  # disable progress bar for better vis in sphinx gallery.
    )

    trainer.train()
    model = trainer.load_best_model()  # load model with best validation PSNR





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 668229 trainable parameters
    /home/runner/work/deepinv/deepinv/deepinv/training/trainer.py:522: UserWarning: Update progress bar frequency of 1 may slow down training on GPU. Consider setting freq_update_progress_bar > 1.
      warnings.warn(
      0%|          | 0/10 [00:00<?, ?it/s]    Train epoch 1/2:   0%|          | 0/10 [00:00<?, ?it/s]    Train epoch 1/2:   0%|          | 0/10 [00:00<?, ?it/s, TotalLoss=9.08e-5, PSNR=40.4]    Train epoch 1/2:  10%|█         | 1/10 [00:00<00:06,  1.43it/s, TotalLoss=9.08e-5, PSNR=40.4]    Train epoch 1/2:  10%|█         | 1/10 [00:00<00:06,  1.43it/s, TotalLoss=9.08e-5, PSNR=40.4]    Train epoch 1/2:  10%|█         | 1/10 [00:01<00:06,  1.43it/s, TotalLoss=0.000776, PSNR=34.4]    Train epoch 1/2:  20%|██        | 2/10 [00:01<00:05,  1.45it/s, TotalLoss=0.000776, PSNR=34.4]    Train epoch 1/2:  20%|██        | 2/10 [00:01<00:05,  1.45it/s, TotalLoss=0.000776, PSNR=34.4]    Train epoch 1/2:  20%|██        | 2/10 [00:02<00:05,  1.45it/s, TotalLoss=0.000697, PSNR=33.8]    Train epoch 1/2:  30%|███       | 3/10 [00:02<00:04,  1.46it/s, TotalLoss=0.000697, PSNR=33.8]    Train epoch 1/2:  30%|███       | 3/10 [00:02<00:04,  1.46it/s, TotalLoss=0.000697, PSNR=33.8]    Train epoch 1/2:  30%|███       | 3/10 [00:02<00:04,  1.46it/s, TotalLoss=0.00213, PSNR=30.8]     Train epoch 1/2:  40%|████      | 4/10 [00:02<00:04,  1.47it/s, TotalLoss=0.00213, PSNR=30.8]    Train epoch 1/2:  40%|████      | 4/10 [00:02<00:04,  1.47it/s, TotalLoss=0.00213, PSNR=30.8]    Train epoch 1/2:  40%|████      | 4/10 [00:03<00:04,  1.47it/s, TotalLoss=0.00313, PSNR=29]      Train epoch 1/2:  50%|█████     | 5/10 [00:03<00:03,  1.47it/s, TotalLoss=0.00313, PSNR=29]    Train epoch 1/2:  50%|█████     | 5/10 [00:03<00:03,  1.47it/s, TotalLoss=0.00313, PSNR=29]    Train epoch 1/2:  50%|█████     | 5/10 [00:04<00:03,  1.47it/s, TotalLoss=0.00386, PSNR=27.7]    Train epoch 1/2:  60%|██████    | 6/10 [00:04<00:02,  1.47it/s, TotalLoss=0.00386, PSNR=27.7]    Train epoch 1/2:  60%|██████    | 6/10 [00:04<00:02,  1.47it/s, TotalLoss=0.00386, PSNR=27.7]    Train epoch 1/2:  60%|██████    | 6/10 [00:04<00:02,  1.47it/s, TotalLoss=0.00385, PSNR=27.2]    Train epoch 1/2:  70%|███████   | 7/10 [00:04<00:02,  1.47it/s, TotalLoss=0.00385, PSNR=27.2]    Train epoch 1/2:  70%|███████   | 7/10 [00:04<00:02,  1.47it/s, TotalLoss=0.00385, PSNR=27.2]    Train epoch 1/2:  70%|███████   | 7/10 [00:05<00:02,  1.47it/s, TotalLoss=0.00402, PSNR=26.6]    Train epoch 1/2:  80%|████████  | 8/10 [00:05<00:01,  1.48it/s, TotalLoss=0.00402, PSNR=26.6]    Train epoch 1/2:  80%|████████  | 8/10 [00:05<00:01,  1.48it/s, TotalLoss=0.00402, PSNR=26.6]    Train epoch 1/2:  80%|████████  | 8/10 [00:06<00:01,  1.48it/s, TotalLoss=0.00519, PSNR=25.7]    Train epoch 1/2:  90%|█████████ | 9/10 [00:06<00:00,  1.48it/s, TotalLoss=0.00519, PSNR=25.7]    Train epoch 1/2:  90%|█████████ | 9/10 [00:06<00:00,  1.48it/s, TotalLoss=0.00519, PSNR=25.7]    Train epoch 1/2:  90%|█████████ | 9/10 [00:06<00:00,  1.48it/s, TotalLoss=0.00512, PSNR=25.5]    Train epoch 1/2: 100%|██████████| 10/10 [00:06<00:00,  1.47it/s, TotalLoss=0.00512, PSNR=25.5]    Train epoch 1/2: 100%|██████████| 10/10 [00:06<00:00,  1.47it/s, TotalLoss=0.00512, PSNR=25.5]
      0%|          | 0/1 [00:00<?, ?it/s]    Eval epoch 1/2:   0%|          | 0/1 [00:00<?, ?it/s]    Eval epoch 1/2:   0%|          | 0/1 [00:00<?, ?it/s, PSNR=20.1]    Eval epoch 1/2: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s, PSNR=20.1]    Eval epoch 1/2: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s, PSNR=20.1]
    Best model saved at epoch 1
      0%|          | 0/10 [00:00<?, ?it/s]    Train epoch 2/2:   0%|          | 0/10 [00:00<?, ?it/s]    Train epoch 2/2:   0%|          | 0/10 [00:00<?, ?it/s, TotalLoss=0.00515, PSNR=22.9]    Train epoch 2/2:  10%|█         | 1/10 [00:00<00:06,  1.46it/s, TotalLoss=0.00515, PSNR=22.9]    Train epoch 2/2:  10%|█         | 1/10 [00:00<00:06,  1.46it/s, TotalLoss=0.00515, PSNR=22.9]    Train epoch 2/2:  10%|█         | 1/10 [00:01<00:06,  1.46it/s, TotalLoss=0.0033, PSNR=25.6]     Train epoch 2/2:  20%|██        | 2/10 [00:01<00:05,  1.48it/s, TotalLoss=0.0033, PSNR=25.6]    Train epoch 2/2:  20%|██        | 2/10 [00:01<00:05,  1.48it/s, TotalLoss=0.0033, PSNR=25.6]    Train epoch 2/2:  20%|██        | 2/10 [00:02<00:05,  1.48it/s, TotalLoss=0.00439, PSNR=24.4]    Train epoch 2/2:  30%|███       | 3/10 [00:02<00:04,  1.47it/s, TotalLoss=0.00439, PSNR=24.4]    Train epoch 2/2:  30%|███       | 3/10 [00:02<00:04,  1.47it/s, TotalLoss=0.00439, PSNR=24.4]    Train epoch 2/2:  30%|███       | 3/10 [00:02<00:04,  1.47it/s, TotalLoss=0.00348, PSNR=26.1]    Train epoch 2/2:  40%|████      | 4/10 [00:02<00:04,  1.48it/s, TotalLoss=0.00348, PSNR=26.1]    Train epoch 2/2:  40%|████      | 4/10 [00:02<00:04,  1.48it/s, TotalLoss=0.00348, PSNR=26.1]    Train epoch 2/2:  40%|████      | 4/10 [00:03<00:04,  1.48it/s, TotalLoss=0.00354, PSNR=25.7]    Train epoch 2/2:  50%|█████     | 5/10 [00:03<00:03,  1.48it/s, TotalLoss=0.00354, PSNR=25.7]    Train epoch 2/2:  50%|█████     | 5/10 [00:03<00:03,  1.48it/s, TotalLoss=0.00354, PSNR=25.7]    Train epoch 2/2:  50%|█████     | 5/10 [00:04<00:03,  1.48it/s, TotalLoss=0.00412, PSNR=25]      Train epoch 2/2:  60%|██████    | 6/10 [00:04<00:02,  1.46it/s, TotalLoss=0.00412, PSNR=25]    Train epoch 2/2:  60%|██████    | 6/10 [00:04<00:02,  1.46it/s, TotalLoss=0.00412, PSNR=25]    Train epoch 2/2:  60%|██████    | 6/10 [00:04<00:02,  1.46it/s, TotalLoss=0.00415, PSNR=24.8]    Train epoch 2/2:  70%|███████   | 7/10 [00:04<00:02,  1.46it/s, TotalLoss=0.00415, PSNR=24.8]    Train epoch 2/2:  70%|███████   | 7/10 [00:04<00:02,  1.46it/s, TotalLoss=0.00415, PSNR=24.8]    Train epoch 2/2:  70%|███████   | 7/10 [00:05<00:02,  1.46it/s, TotalLoss=0.00543, PSNR=24]      Train epoch 2/2:  80%|████████  | 8/10 [00:05<00:01,  1.45it/s, TotalLoss=0.00543, PSNR=24]    Train epoch 2/2:  80%|████████  | 8/10 [00:05<00:01,  1.45it/s, TotalLoss=0.00543, PSNR=24]    Train epoch 2/2:  80%|████████  | 8/10 [00:06<00:01,  1.45it/s, TotalLoss=0.00488, PSNR=25]    Train epoch 2/2:  90%|█████████ | 9/10 [00:06<00:00,  1.46it/s, TotalLoss=0.00488, PSNR=25]    Train epoch 2/2:  90%|█████████ | 9/10 [00:06<00:00,  1.46it/s, TotalLoss=0.00488, PSNR=25]    Train epoch 2/2:  90%|█████████ | 9/10 [00:06<00:00,  1.46it/s, TotalLoss=0.00508, PSNR=24.7]    Train epoch 2/2: 100%|██████████| 10/10 [00:06<00:00,  1.47it/s, TotalLoss=0.00508, PSNR=24.7]    Train epoch 2/2: 100%|██████████| 10/10 [00:06<00:00,  1.47it/s, TotalLoss=0.00508, PSNR=24.7]
      0%|          | 0/1 [00:00<?, ?it/s]    Eval epoch 2/2:   0%|          | 0/1 [00:00<?, ?it/s]    Eval epoch 2/2:   0%|          | 0/1 [00:00<?, ?it/s, PSNR=20.1]    Eval epoch 2/2: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s, PSNR=20.1]    Eval epoch 2/2: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s, PSNR=20.1]
    Best model saved at epoch 2




.. GENERATED FROM PYTHON SOURCE LINES 201-205

Test the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 205-224

.. code-block:: Python


    trainer.test(test_dataloader)

    test_sample, _ = next(iter(test_dataloader))
    model.eval()
    test_sample = test_sample.to(device)

    # Get the measurements and the ground truth
    y = physics(test_sample)
    with torch.no_grad():
        rec = model(y, physics=physics)

    backprojected = physics.A_adjoint(y)

    dinv.utils.plot(
        [backprojected, rec, test_sample],
        titles=["Linear", "Reconstruction", "Ground truth"],
        suptitle="Reconstruction results",
    )



.. image-sg:: /auto_examples/unfolded/images/sphx_glr_demo_DEQ_001.png
   :alt: Reconstruction results, Linear, Reconstruction, Ground truth
   :srcset: /auto_examples/unfolded/images/sphx_glr_demo_DEQ_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/1 [00:00<?, ?it/s]    Test:   0%|          | 0/1 [00:00<?, ?it/s]    Test:   0%|          | 0/1 [00:00<?, ?it/s, PSNR=20.1, PSNR no learning=17]    Test: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s, PSNR=20.1, PSNR no learning=17]    Test: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s, PSNR=20.1, PSNR no learning=17]
    Test results:
    PSNR no learning: 16.952 +- 0.651
    PSNR: 20.120 +- 1.493
    /home/runner/work/deepinv/deepinv/deepinv/utils/plotting.py:379: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.
      fig.subplots_adjust(top=0.75)





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 16.964 seconds)


.. _sphx_glr_download_auto_examples_unfolded_demo_DEQ.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_DEQ.ipynb <demo_DEQ.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_DEQ.py <demo_DEQ.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: demo_DEQ.zip <demo_DEQ.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
